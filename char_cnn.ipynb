{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-level Convolutional Networks for TextClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3000\n",
    "conv_layers = [[256, 7, 3],\n",
    "               [256, 7, 3],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, 3]]\n",
    "\n",
    "fully_connected_layers = [1024, 1024]\n",
    "num_of_classes = 5\n",
    "dropout_p = 0.5\n",
    "optimizer = 'adam'\n",
    "loss_type = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_cnn_model(text, labels, num_epochs):\n",
    "    \n",
    "    tk = Tokenizer(lower=True, char_level=True, oov_token='UNK')\n",
    "    tk.fit_on_texts(text)\n",
    "    sequences = tk.texts_to_sequences(text)\n",
    "    \n",
    "    data = pad_sequences(sequences, maxlen=input_size)\n",
    "    labels = to_categorical(labels)\n",
    "    \n",
    "    vocab_size = len(tk.word_index)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size = 0.2, random_state = 42)\n",
    "\n",
    "    #creating embedding matrix\n",
    "    embedding_weights = []\n",
    "    embedding_weights.append(np.zeros(vocab_size))\n",
    "\n",
    "    for char, i in tk.word_index.items():\n",
    "        onehot = np.zeros(vocab_size)\n",
    "        onehot[i - 1] = 1\n",
    "        embedding_weights.append(onehot)\n",
    "\n",
    "    embedding_weights = np.array(embedding_weights)\n",
    "\n",
    "    embedding_layer = Embedding(vocab_size + 1, vocab_size, input_length=input_size, weights=[embedding_weights])\n",
    "\n",
    "    #Model architecture\n",
    "    inputs = Input(shape=(input_size,), name='input', dtype='int64')\n",
    "    x = embedding_layer(inputs)\n",
    "    \n",
    "    for filter_num, filter_size, pooling_size in conv_layers:\n",
    "        x = Conv1D(filter_num, filter_size)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        if pooling_size != -1:\n",
    "            x = MaxPooling1D(pool_size=pooling_size)(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    for dense_size in fully_connected_layers:\n",
    "        x = Dense(dense_size, activation='relu')(x)\n",
    "        x = Dropout(dropout_p)(x)\n",
    "    \n",
    "    predictions = Dense(num_of_classes, activation='softmax')(x)\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer=optimizer, loss=loss_type, metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=num_epochs, batch_size=8)\n",
    "    \n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        category                                               text\n",
      "0           tech  tv future in the hands of viewers with home th...\n",
      "1       business  worldcom boss  left books alone  former worldc...\n",
      "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
      "3          sport  yeading face newcastle in fa cup premiership s...\n",
      "4  entertainment  ocean s twelve raids box office ocean s twelve...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('bbc-text.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(df['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aman/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "1780/1780 [==============================] - 495s 278ms/step - loss: 1.5838 - acc: 0.2551\n",
      "Epoch 2/5\n",
      "1780/1780 [==============================] - 469s 264ms/step - loss: 1.5248 - acc: 0.2944\n",
      "Epoch 3/5\n",
      "1780/1780 [==============================] - 467s 262ms/step - loss: 1.5154 - acc: 0.3011\n",
      "Epoch 4/5\n",
      "1780/1780 [==============================] - 475s 267ms/step - loss: 1.3277 - acc: 0.4118\n",
      "Epoch 5/5\n",
      "1780/1780 [==============================] - 522s 293ms/step - loss: 1.0560 - acc: 0.5567\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = char_cnn_model(df['text'], labels, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.393259\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
